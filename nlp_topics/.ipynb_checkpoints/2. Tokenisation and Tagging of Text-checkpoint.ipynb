{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation and Tagging of Text\n",
    "In order to classify and analyze a body of text in a more granular fashion, it is necessary to consider how to break it into individual sentences and words or \"tokens\". Broadly then there are two tasks:\n",
    "* Sentence Tokenization\n",
    "* Word Tokenization\n",
    "\n",
    "To go beyond counting the frequency or occurence of actual words we need to classify words in general categories that signify their part in the construct of the sentence - for instance Noun, Verb Adjective etc. This is generally known as\n",
    "* Part of Speech or POS Tagging\n",
    "\n",
    "# Token\n",
    "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenisation\n",
    "The default Sentence Tokenizer is the `PunktSentenceTokenizer` from the `nltk.tokenize.punkt` module.\n",
    "\n",
    "\n",
    "In the example below (taken from James Joyce's Ulysses), we load the nltk library and process a block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mrkgnao!\n",
      "> the cat said loudly.\n",
      "> She blinked up out of her avid shameclosing eyes, mewing plaintively and long, showing him her milkwhite teeth.\n",
      "> He watched the dark eyeslits narrowing with greed till her eyes were green stones.\n",
      "> Then he went to the dresser, took the jug Hanlon'smilkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.— Gurrhr!\n",
      "> she cried, running to lap.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "ulysses = \"Mrkgnao! the cat said loudly. She blinked up out of her avid shameclosing eyes, mewing \\\n",
    "plaintively and long, showing him her milkwhite teeth. He watched the dark eyeslits narrowing \\\n",
    "with greed till her eyes were green stones. Then he went to the dresser, took the jug Hanlon's\\\n",
    "milkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.\\\n",
    "— Gurrhr! she cried, running to lap.\"\n",
    "\n",
    "doc = nltk.sent_tokenize(ulysses)\n",
    "for s in doc:\n",
    "    print(\">\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word Tokenisation\n",
    "There are many methods for tokenising text into words. The default Penn Treebank Tokeniser is the tokeniser based on the Penn TreeBank Corpus. A few examples of different tokenisers giving different results are listed below:\n",
    "* TreebankWordTokenizer\n",
    "* WordPunctTokenizer\n",
    "* WhitespaceTokenize\n",
    "\n",
    "We can see a simple illustration of the impact of chosing a different tokenisation method by looking at the different results we get for a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT:  ['Mary', 'had', 'a', 'little', 'lamb', 'It', \"'s\", 'fleece', 'was', 'white', 'as', 'snow', '.']\n",
      "\n",
      "PUNCT  :  ['Mary', 'had', 'a', 'little', 'lamb', 'It', \"'\", 's', 'fleece', 'was', 'white', 'as', 'snow', '.']\n",
      "\n",
      "SPACE  :  ['Mary', 'had', 'a', 'little', 'lamb', \"It's\", 'fleece', 'was', 'white', 'as', 'snow.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = \"Mary had a little lamb It's fleece was white as snow.\"\n",
    "\n",
    "# Default Tokenization \n",
    "tree_tokens = word_tokenize(sentence)\n",
    "\n",
    "#other Tokenizers\n",
    "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(sentence)\n",
    "\n",
    "space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "space_tokens = space_tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"DEFAULT: \", tree_tokens)\n",
    "print()\n",
    "print(\"PUNCT  : \", punct_tokens)\n",
    "print()\n",
    "print(\"SPACE  : \", space_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon and corporas\n",
    "- Corpora : body of text of same thing, ex: medical journals, presidental speeches\n",
    "- Lexicon : Words and their meanings\n",
    "    ex: investor-speak vs regular english-speak\n",
    "    - investor speak 'bull' = someone who is positive about the market\n",
    "    - english speak 'bull' scary animal you donot want runing @ you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is grate and Python is awesome.The sky is pinkish-blue.', \"you shouldn't eat cardboard\"]\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'grate', 'and', 'Python', 'is', 'awesome.The', 'sky', 'is', 'pinkish-blue', '.', 'you', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "example_text = \"Hello Mr. Smith, how are you doing today? The weather is grate and Python is awesome.\\\n",
    "The sky is pinkish-blue. you shouldn't eat cardboard\" \n",
    "\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note here. First, notice that punctuation is treated as a separate token. Also, notice the separation of the word \"shouldn't\" into \"should\" and \"n't.\" Finally, notice that \"pinkish-blue\" is indeed treated like the \"one word\" it was meant to be turned into.\n",
    "\n",
    "We start to ponder about how might we derive meaning by looking at these words. We can clearly think of ways to put value to many words, but we also see a few words that are basically worthless. These are a form of \"stop words,\" which we can also handle for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "\n",
    "One of the more powerful aspects of the NLTK module is the Part of Speech tagging that it can do for you. This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more.\n",
    "\n",
    "For each word-token the nltk pos_tag method can be used to classify its Part of Speech (POS), automating the classification of words into their parts of speech and labeling them accordingly.\n",
    "\n",
    "\n",
    "The outcome depends on how the sentence has been split up into individual tokens and which Tokensizer and Corpus the POS-tagger has been trained against:\n",
    "\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPOS tag list:\\n\\nCC\\tcoordinating conjunction\\nCD\\tcardinal digit\\nDT\\tdeterminer\\nEX\\texistential there (like: \"there is\" ... think of it like \"there exists\")\\nFW\\tforeign word\\nIN\\tpreposition/subordinating conjunction\\nJJ\\tadjective\\t\\'big\\'\\nJJR\\tadjective, comparative\\t\\'bigger\\'\\nJJS\\tadjective, superlative\\t\\'biggest\\'\\nLS\\tlist marker\\t1)\\nMD\\tmodal\\tcould, will\\nNN\\tnoun, singular \\'desk\\'\\nNNS\\tnoun plural\\t\\'desks\\'\\nNNP\\tproper noun, singular\\t\\'Harrison\\'\\nNNPS\\tproper noun, plural\\t\\'Americans\\'\\nPDT\\tpredeterminer\\t\\'all the kids\\'\\nPOS\\tpossessive ending\\tparent\\'s\\nPRP\\tpersonal pronoun\\tI, he, she\\nPRP$\\tpossessive pronoun\\tmy, his, hers\\nRB\\tadverb\\tvery, silently,\\nRBR\\tadverb, comparative\\tbetter\\nRBS\\tadverb, superlative\\tbest\\nRP\\tparticle\\tgive up\\nTO\\tto\\tgo \\'to\\' the store.\\nUH\\tinterjection\\terrrrrrrrm\\nVB\\tverb, base form\\ttake\\nVBD\\tverb, past tense\\ttook\\nVBG\\tverb, gerund/present participle\\ttaking\\nVBN\\tverb, past participle\\ttaken\\nVBP\\tverb, sing. present, non-3d\\ttake\\nVBZ\\tverb, 3rd person sing. present\\ttakes\\nWDT\\twh-determiner\\twhich\\nWP\\twh-pronoun\\twho, what\\nWP$\\tpossessive wh-pronoun\\twhose\\nWRB\\twh-abverb\\twhere, when\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'NN'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow', 'NN'), ('.', '.')]\n",
      "[('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'JJ'), (\"It's\", 'NNP'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos = nltk.pos_tag(tree_tokens)\n",
    "print(pos)\n",
    "pos_space = nltk.pos_tag(space_tokens)\n",
    "print(pos_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naming convention of the PoS tags makes it easy to use regular expressions to extract classes of word-type (i.e. all the Nouns or Verbs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['Mary', 'lamb', 'fleece', 'snow']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile(\"^N.*\")\n",
    "nouns = []\n",
    "for l in pos:\n",
    "    if regex.match(l[1]):\n",
    "        nouns.append(l[0])\n",
    "print(\"Nouns:\", nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('February', 'NNP'), ('2', 'CD'), (',', ','), ('2005', 'CD'), ('9:10', 'CD'), ('P.M.', 'NNP'), ('EST', 'NNP'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('As', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('Congress', 'NNP'), ('gathers', 'NNS'), (',', ','), ('all', 'DT'), ('of', 'IN'), ('us', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('elected', 'JJ'), ('branches', 'NNS'), ('of', 'IN'), ('government', 'NN'), ('share', 'NN'), ('a', 'DT'), ('great', 'JJ'), ('privilege', 'NN'), (':', ':'), ('We', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('placed', 'VBN'), ('in', 'IN'), ('office', 'NN'), ('by', 'IN'), ('the', 'DT'), ('votes', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('people', 'NNS'), ('we', 'PRP'), ('serve', 'VBP'), ('.', '.')]\n",
      "[('And', 'CC'), ('tonight', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('a', 'DT'), ('privilege', 'NN'), ('we', 'PRP'), ('share', 'NN'), ('with', 'IN'), ('newly-elected', 'JJ'), ('leaders', 'NNS'), ('of', 'IN'), ('Afghanistan', 'NNP'), (',', ','), ('the', 'DT'), ('Palestinian', 'JJ'), ('Territories', 'NNP'), (',', ','), ('Ukraine', 'NNP'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('free', 'JJ'), ('and', 'CC'), ('sovereign', 'JJ'), ('Iraq', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('Two', 'CD'), ('weeks', 'NNS'), ('ago', 'RB'), (',', ','), ('I', 'PRP'), ('stood', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('steps', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('Capitol', 'NNP'), ('and', 'CC'), ('renewed', 'VBN'), ('the', 'DT'), ('commitment', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('nation', 'NN'), ('to', 'TO'), ('the', 'DT'), ('guiding', 'VBG'), ('ideal', 'NN'), ('of', 'IN'), ('liberty', 'NN'), ('for', 'IN'), ('all', 'DT'), ('.', '.')]\n",
      "[('This', 'DT'), ('evening', 'NN'), ('I', 'PRP'), ('will', 'MD'), ('set', 'VB'), ('forth', 'JJ'), ('policies', 'NNS'), ('to', 'TO'), ('advance', 'VB'), ('that', 'DT'), ('ideal', 'NN'), ('at', 'IN'), ('home', 'NN'), ('and', 'CC'), ('around', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatizing\n",
    "\n",
    "Striping off the suffixes from words is known as stemming.\n",
    "Mapping a word to a known dictionary word is know as lemmatization\n",
    "\n",
    "There are multiple Stemming methods available and the the NLTK book references a few methods in particular:\n",
    "* The Porter Stemmer - see https://tartarus.org/martin/PorterStemmer/\n",
    "* Lancaster Stemmer - (Chris Paice, University of Lancaster) additionally the\n",
    "* Snowball Stemmer - \"Porter 2\" developed by Martin Porter is generally considered the de-facto optimal Stemmer\n",
    "\n",
    "A list of other stemming methods can be found here: http://www.nltk.org/api/nltk.stem.html. Current Stemming and \"Lemming\" techniques are an inexact process as things currently stand.\n",
    "\n",
    "### Stemming Example\n",
    "\n",
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "\n",
    "Consider:\n",
    "\n",
    "- I was taking a ride in the car.\n",
    "- I was riding in the car.\n",
    "\n",
    "This sentence means the same thing\n",
    "\n",
    "One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mari', 'had', 'a', 'littl', 'lamb', 'It', \"'s\", 'fleec', 'wa', 'white', 'as', 'snow', '.']\n",
      "\n",
      "['mary', 'had', 'a', 'littl', 'lamb', 'it', \"'s\", 'fleec', 'was', 'whit', 'as', 'snow', '.']\n",
      "\n",
      "['mari', 'had', 'a', 'littl', 'lamb', 'it', \"'s\", 'fleec', 'was', 'white', 'as', 'snow', '.']\n",
      "\n",
      " When I was going into the woods I saw a bear lying asleep on the forest floor\n",
      "['When', 'I', 'wa', 'go', 'into', 'the', 'wood', 'I', 'saw', 'a', 'bear', 'lie', 'asleep', 'on', 'the', 'forest', 'floor']\n",
      "['when', 'i', 'was', 'going', 'into', 'the', 'wood', 'i', 'saw', 'a', 'bear', 'lying', 'asleep', 'on', 'the', 'forest', 'flo']\n",
      "['when', 'i', 'was', 'go', 'into', 'the', 'wood', 'i', 'saw', 'a', 'bear', 'lie', 'asleep', 'on', 'the', 'forest', 'floor']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "print([porter.stem(t) for t in tree_tokens])\n",
    "print()\n",
    "print([lancaster.stem(t) for t in tree_tokens])\n",
    "print()\n",
    "print([snowball.stem(t) for t in tree_tokens])\n",
    "\n",
    "sentence2 = \"When I was going into the woods I saw a bear lying asleep on the forest floor\"\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "\n",
    "print(\"\\n\",sentence2)\n",
    "for stemmer in [porter, lancaster, snowball]:\n",
    "    print([stemmer.stem(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Example\n",
    "\n",
    "Lemmatization aims to achieve a similar base \"stem\" for a word, but aims to derive the genuine dictionary root word, not just a trunctated version of the word.\n",
    "\n",
    "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words, whereas lemmas are actual words.\n",
    "\n",
    "So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma.\n",
    "\n",
    "Some times you will wind up with a very similar word, but sometimes, you will wind up with a completely different word. Let's see some examples.\n",
    "\n",
    "The default lemmatization method with the Python NLTK is the WordNet lemmatizer.\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb', 'It', \"'s\", 'fleece', 'wa', 'white', 'a', 'snow', '.']\n",
      "['When', 'I', 'wa', 'going', 'into', 'the', 'wood', 'I', 'saw', 'a', 'bear', 'lying', 'asleep', 'on', 'the', 'forest', 'floor']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "tokens2_pos = nltk.pos_tag(tokens2)  #nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "print([wnl.lemmatize(t) for t in tree_tokens])\n",
    "\n",
    "print([wnl.lemmatize(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've got a bunch of examples of the lemma for the words that we use. The only major thing to note is that lemmatize takes a part of speech parameter, \"pos.\" If not supplied, the default is \"noun.\" This means that an attempt will be made to find the closest noun, which can create trouble for you. Keep this in mind if you use lemmatizing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "By Tokenising text into sentences and words we can go beyond counting the frequency or occurence of actual words and instead classify words by a classification type (i.e. we can identify common features in the text).\n",
    "\n",
    "## Further Investigation\n",
    "Optional further work and experimentation:\n",
    "\n",
    "* **Regular Expressions and POS patterns**\n",
    "\n",
    "Consider how to extract \"phrase-chunks\" based on regular expressions.\n",
    "\n",
    "See this Stack Overflow thread for one idea: http://stackoverflow.com/questions/34090734/how-to-use-nltk-regex-pattern-to-extract-a-specific-phrase-chunk\n",
    "\n",
    "Consider the pitfalls and complexities for different sentence constructs with essentially the same meaning.\n",
    "* **Experiment with the POS Tags**\n",
    "\n",
    "Try tokenising the sentance:\n",
    "\"When I was going into the woods I saw a bear lying asleep on the forest floor\"\n",
    "and note any inaccuriacies in the PoS classifications."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
